---
# Documentation: https://wowchemy.com/docs/managing-content/

title: Egoshots, an ego-vision life-logging dataset and semantic fidelity metric to
  evaluate diversity in image captioning models
subtitle: ''
summary: ''
authors:
- Pranav Agarwal
- Alejandro Betancourt
- Vana Panagiotou
- Natalia Díaz-Rodríguez
tags:
- '"Computer Science - Computer Vision and Pattern Recognition"'
- '"Computer Science - Machine Learning"'
categories: []
date: '2020-03-01'
lastmod: 2020-12-21T16:04:01+05:30
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2020-12-21T10:34:01.764315Z'
publication_types:
- '1'
abstract: Image captioning models have been able to generate grammatically correct
  and human understandable sentences. However most of the captions convey limited
  information as the model used is trained on datasets that do not caption all possible
  objects existing in everyday life. Due to this lack of prior information most of
  the captions are biased to only a few objects present in the scene, hence limiting
  their usage in daily life. In this paper, we attempt to show the biased nature of
  the currently existing image captioning models and present a new image captioning
  dataset, Egoshots, consisting of 978 real life images with no captions. We further
  exploit the state of the art pre-trained image captioning and object recognition
  networks to annotate our images and show the limitations of existing works. Furthermore,
  in order to evaluate the quality of the generated captions, we propose a new image
  captioning metric, object based Semantic Fidelity (SF). Existing image captioning
  metrics can evaluate a caption only in the presence of their corresponding annotations;
  however, SF allows evaluating captions generated for images without annotations,
  making it highly useful for real life generated captions.

publication: 'Proceedings of the 8th International Conference on Learning Representations (ICLR) 2020'
publication_short: Presented at the ICLR Machine Learning in Real Life Workshop 2020
url_pdf: http://arxiv.org/abs/2003.11743
url_video: 'https://www.youtube.com/watch?v=TFzxFfI90sc'
url_slides: 'https://docs.google.com/presentation/d/1UOadIVy_CYFsFC5POjfgUxvwxST35P6kDXUp2Au4kVI/edit#slide=id.p'
---
